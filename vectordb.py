# -*- coding: utf-8 -*-
"""VectorDB.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KazZvAOtZRWAgJE3C4iklp6QE2JaJsFw
"""

!pip -q install chromadb pandas scikit-learn

# For setting up the vector db
import chromadb

# For working with our data files
import os
import glob

# For handling our data
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# Miscellaneous
import time # to see how long code processes take to run
import textwrap # to pretty print abstracts

!gdown 1g3K-wlixFxklTSUQNZKpEgN4WNTFTPIZ
# https://drive.google.com/file/d/1g3K-wlixFxklTSUQNZKpEgN4WNTFTPIZ/view
# https://alex.macrocosm.so/download

!unzip arxiv_abstracts.zip

def load_parquet_file(file_path: str) -> pd.DataFrame:
    return pd.read_parquet(file_path)

data_df = None
i = 0
for file_path in glob.glob(os.path.join('arxiv_abstracts/', 'abstracts_*.parquet')):
    print('Loading data from {}'.format(file_path))
    file_df = load_parquet_file(file_path)

    if data_df is None:
        # initialize the dataframe
        data_df = file_df
    else:
        # append the new data to our dataframe
        data_df = pd.concat([data_df, file_df], ignore_index=True)

    # Use this to limit the amount of data
    # Each file contains 100,000 rows
    i += 1
    if i >= 5:
        break

# A little data prep
data_df = data_df.rename(columns={'embeddings': 'embedding'})
data_df['id'] = data_df.index.astype(str)

data_df.head()

user_liked_paper = data_df[data_df['doi'] == '1911.01406'].iloc[0].to_dict()
print(f"abstract: \n{user_liked_paper['abstract']}")

def brute_force_search(user_liked_paper, data_df, num_results=5):
    start_time = time.time()

    data_df['similarity'] = data_df['embedding'].apply(
		    lambda x: cosine_similarity([user_liked_paper['embedding']], [x])[0][0]
	  )

    # Make sure the results don't contain the same paper as the query
    search_results = data_df[data_df['doi'] != user_liked_paper['doi']].sort_values(
		    by='similarity',
		    ascending=False
	  ).head(num_results)

    print(f"Time taken to brute force search: {round(time.time() - start_time, 3)} seconds")

    # Select and reorder columns, reset index
    search_results = search_results[["id", "abstract", "doi"]].reset_index(
		    drop=True
	  )

    return search_results

search_results = brute_force_search(user_liked_paper, data_df, num_results=5)
display(search_results)

db_client = chromadb.PersistentClient(path="chroma_db")

collection_name = "paper_abstracts"

if collection_name in db_client.list_collections():
    db_client.delete_collection(name=collection_name)
    print('Dropped existing collection to remake')

abstract_collection = db_client.create_collection(
    name=collection_name,
    # define the distance metric to use when comparing vectors in this collection
    metadata={"hnsw:space": "cosine"})

batch_size = 1000
for i in range(0, len(data_df), batch_size):
    data_batch = data_df.iloc[i:i+batch_size]
    abstract_collection.add(
        ids=data_batch['id'].tolist(),
        embeddings=data_batch['embedding'].tolist(),
        documents=data_batch['abstract'].tolist(),
        metadatas=data_batch[['doi']].to_dict(orient='records')
    )

def vector_db_search(user_liked_paper, collection_name, num_results):
    start_time = time.time()

    search_results = abstract_collection.query(
        query_embeddings=[user_liked_paper['embedding']],
        n_results=num_results,
        # Make sure the results don't contain the same paper as the query
        where={"doi": {"$ne": user_liked_paper['doi']}}
    )

    print(f"Time taken to search vector database: {round(time.time() - start_time, 3)} seconds")

    # Create a pandas dataframe from the search results
    search_results_df = pd.DataFrame(search_results["ids"][0], columns=["id"])
    search_results_df["document"] = search_results["documents"][0]
    search_results_df["doi"] = [meta["doi"] for meta in search_results["metadatas"][0]]
    search_results_df = search_results_df.rename(columns={"document": "abstract"})

    return search_results_df


search_results_df = vector_db_search(user_liked_paper, "article_abstracts", 5)
display(search_results_df)